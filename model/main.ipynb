{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "DATASET_LOCATION = 'data/ml-latest-small'\n",
    "PARAMETER_FILE = 'parameters/model_small.pth'\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def load_data():\n",
    "    movies = pd.read_csv(f'{DATASET_LOCATION}/movies.csv')\n",
    "    ratings = pd.read_csv(f'{DATASET_LOCATION}/ratings.csv')\n",
    "    return movies, ratings\n",
    "\n",
    "def preprocess_data(ratings):\n",
    "    user_ids = ratings['userId'].unique()\n",
    "    movie_ids = ratings['movieId'].unique()\n",
    "\n",
    "    user_to_index = {user_id: index for index, user_id in enumerate(user_ids)}\n",
    "    movie_to_index = {movie_id: index for index, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "    ratings['userId'] = ratings['userId'].map(user_to_index)\n",
    "    ratings['movieId'] = ratings['movieId'].map(movie_to_index)\n",
    "\n",
    "    train_data, test_data = train_test_split(ratings, test_size=0.2)\n",
    "    return train_data, test_data, len(user_ids), len(movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.user_embedding(user_indices)\n",
    "        item_embedding = self.item_embedding(item_indices)\n",
    "        return (user_embedding * item_embedding).sum(1)\n",
    "    \n",
    "    def get_item_embedding(self, item_index):\n",
    "        return self.item_embedding(item_index)\n",
    "\n",
    "# Dataset Definition\n",
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ratings.iloc[idx]\n",
    "        return row['userId'], row['movieId'], row['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_model(train_data, num_users, num_items, embedding_dim=50, epochs=10, lr=0.01):\n",
    "    model = MatrixFactorization(num_users, num_items, embedding_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_dataset = RatingsDataset(train_data)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for user_indices, item_indices, ratings in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            user_indices = user_indices.long().to(device)\n",
    "            item_indices = item_indices.long().to(device)\n",
    "            ratings = ratings.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(user_indices, item_indices)\n",
    "            loss = criterion(outputs, ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), PARAMETER_FILE)\n",
    "    return model\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, test_data):\n",
    "    model.eval()\n",
    "    test_dataset = RatingsDataset(test_data)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for user_indices, item_indices, ratings in test_loader:\n",
    "            user_indices = user_indices.long().to(device)\n",
    "            item_indices = item_indices.long().to(device)\n",
    "            ratings = ratings.float().to(device)\n",
    "\n",
    "            outputs = model(user_indices, item_indices)\n",
    "            loss = criterion(outputs, ratings)\n",
    "            total_loss += loss.item()\n",
    "    print(f'Test Loss: {total_loss/len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar movies\n",
    "def find_similar_movies(model, movie_id, movie_to_index, index_to_movie, top_k=5):\n",
    "    model.eval()\n",
    "    movie_index = torch.tensor([movie_to_index[movie_id]]).to(device)\n",
    "    movie_embedding = model.get_item_embedding(movie_index).detach().cpu().numpy()\n",
    "\n",
    "    all_movie_embeddings = model.item_embedding.weight.detach().cpu().numpy()\n",
    "    similarities = np.dot(all_movie_embeddings, movie_embedding.T).flatten()\n",
    "    similar_movie_indices = similarities.argsort()[-top_k-1:-1][::-1]\n",
    "\n",
    "    similar_movies = [index_to_movie[idx] for idx in similar_movie_indices]\n",
    "    return similar_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1261/1261 [00:07<00:00, 160.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 37.272869165316166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1261/1261 [00:07<00:00, 167.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 6.9295042391148565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1261/1261 [00:07<00:00, 167.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 2.10066028779504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1261/1261 [00:07<00:00, 169.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 1.6063616857180796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1261/1261 [00:07<00:00, 172.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 2.028037469364366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1261/1261 [00:07<00:00, 171.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 2.0816815279477368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1261/1261 [00:07<00:00, 173.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 1.584652452833779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1261/1261 [00:07<00:00, 168.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 1.2783865281081597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1261/1261 [00:07<00:00, 179.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 1.2089920645191214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1261/1261 [00:06<00:00, 182.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 1.1393502782867788\n",
      "Test Loss: 2.5475297196756435\n",
      "Error processing movie 190183: index out of range in self\n",
      "Error processing movie 191005: index out of range in self\n",
      "Error processing movie 193565: index out of range in self\n",
      "Error processing movie 193567: index out of range in self\n",
      "Error processing movie 193571: index out of range in self\n",
      "Error processing movie 193573: index out of range in self\n",
      "Error processing movie 193579: index out of range in self\n",
      "Error processing movie 193581: index out of range in self\n",
      "Error processing movie 193583: index out of range in self\n",
      "Error processing movie 193585: index out of range in self\n",
      "Error processing movie 193587: index out of range in self\n",
      "Error processing movie 193609: index out of range in self\n",
      "Error processing movie 190207: index out of range in self\n",
      "Error processing movie 190209: index out of range in self\n",
      "Error processing movie 190213: index out of range in self\n",
      "Error processing movie 190215: index out of range in self\n",
      "Error processing movie 190219: index out of range in self\n",
      "Error processing movie 190221: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    movies, ratings = load_data()\n",
    "    train_data, test_data, num_users, num_items = preprocess_data(ratings)\n",
    "\n",
    "    model = None\n",
    "\n",
    "    if not os.path.isfile(PARAMETER_FILE):\n",
    "        model = train_model(train_data, num_users, num_items, embedding_dim=50)\n",
    "    else:\n",
    "        model = MatrixFactorization(num_users, num_items, embedding_dim=50)\n",
    "    \n",
    "    # Load the model for evaluation\n",
    "    model.load_state_dict(torch.load(PARAMETER_FILE, weights_only=True))\n",
    "    model.to(device)\n",
    "    evaluate_model(model, test_data)\n",
    "    \n",
    "    # Create index to movie mapping\n",
    "    movie_to_index = {movie_id: index for index, movie_id in enumerate(movies['movieId'].unique())}\n",
    "    index_to_movie = {index: movie_id for movie_id, index in movie_to_index.items()}\n",
    "\n",
    "    all_movie_ids = pd.read_csv(f'{DATASET_LOCATION}/ratings.csv')[\"movieId\"].unique()\n",
    "\n",
    "    recommendations = {}\n",
    "\n",
    "    links = pd.read_csv(f'{DATASET_LOCATION}/links.csv')\n",
    "    links['tmdbId'] = links['tmdbId'].fillna(0).astype(int)\n",
    "\n",
    "for id in all_movie_ids:\n",
    "    if id in movie_to_index:\n",
    "        try:\n",
    "            tmdb_id = int(links.loc[links['movieId'] == id]['tmdbId'].values[0])\n",
    "            current_recommendation = find_similar_movies(model, id, movie_to_index, index_to_movie, top_k=15)\n",
    "            current_tmdb_recommendation = [links.loc[links['movieId'] == movie_id]['tmdbId'].values[0] for movie_id in current_recommendation]\n",
    "            recommendations[tmdb_id] = list(map(int, current_tmdb_recommendation))\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError processing movie {id}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing movie {id}: {e}\")\n",
    "    else:\n",
    "        print(f\"Movie ID {id} not found in movie_to_index\")\n",
    "\n",
    "with open(\"../app/recommendations/recommendations.json\", \"w\") as f:\n",
    "    json.dump(recommendations, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
